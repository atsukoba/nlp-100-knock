{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 言語処理100本ノック\n",
    "# 第5章: 係り受け解析\n",
    "\n",
    "---\n",
    "\n",
    "夏目漱石の小説『吾輩は猫である』の文章（`neko.txt`）を`CaboCha`を使って係り受け解析し，その結果を`neko.txt.cabocha`というファイルに保存せよ．このファイルを用いて，以下の問に対応するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Cabocha` : [レファレンス](http://chasen.naist.jp/chaki/t/2005-08-29/doc/CaboCha%20Yet%20Another%20Japanese%20Dependency%20Structure%20Analyzer.htm)\n",
    "\n",
    ">Support Vector Machines (SVMs) に基づく, 高性能な係り受け解析器  \n",
    ">SVM の分類アルゴリズムの高速化手法である PKE (ACL 2003 にて発表)を適用.  \n",
    ">決定的な解析アルゴリズム (Cascaded Chunking Model) を採用. 高効率な解析  \n",
    ">係り関係そのものを素性として考慮する「動的素性」を採用, 精度向上に大きく貢献  \n",
    ">文節の区切にも SVMs を採用 (実際には YamChaを使用)  \n",
    ">単純な並列/同格構造解析が可能  \n",
    ">IREX の定義による固有表現解析が可能  \n",
    ">柔軟な入力形式. 生文はもちろん, 形態素解析済みデータ, 文節区切り済み データ, 部分的に係り関係が付与された>データからの解析が可能  \n",
    ">係り受けの同定に使用する素性をユーザ側で再定義可能  \n",
    ">データを用意すれば, ユーザ側で学習を行うことが可能  \n",
    ">内部の辞書に, 高速な Trie 構造である Double-Array を採用  \n",
    ">1文/秒程度の現実的な解析速度  \n",
    ">C/C++/Perl/Ruby ライブラリの提供  \n",
    "\n",
    "- [Cabocha Documentation](http://taku910.github.io/cabocha/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lattice（CABOCHA_FORMAT_LATTICE） 型で解析を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neko.txt.cabocha has already been exist\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"neko.txt.cabocha\"):\n",
    "    print(\"neko.txt.cabocha has already been exist\")\n",
    "else:\n",
    "    os.system(\"cabocha -f1 neko.txt > neko.txt.cabocha\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Cabocha`のインストール (必要ならば)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % brew install cabosha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ヘルパー系関数共"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = [\"\\u3000\",\" \", \"、\", \"。\", \",\", \".\"]\n",
    "\n",
    "def is_not_symbol(string: str) -> bool:\n",
    "    symbol = 0\n",
    "    symbols = [\"\\u3000\",\" \", \"、\", \"。\", \",\", \".\"]\n",
    "    for s in symbols:\n",
    "        symbol += s in string\n",
    "    return symbol == 0\n",
    "\n",
    "def rm_symbol(sentence: list) -> list:\n",
    "    for s in symbols:\n",
    "        if s in sentence:\n",
    "            sentence.remove(s)\n",
    "    return sentence\n",
    "\n",
    "def check(l: list, head=5) -> None:\n",
    "    output = \"\"\n",
    "    output += \"Length: {}\\n\".format(len(l))\n",
    "    output += \"First {} elements: {}\".format(head, l[:head])\n",
    "    print(output)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40. 係り受け解析結果の読み込み（形態素）\n",
    "\n",
    "---\n",
    "\n",
    "形態素を表すクラス`Morph`を実装せよ．このクラスは表層形（`surface`），基本形（`base`），品詞（`pos`），品詞細分類1（`pos1`）をメンバ変数に持つこととする．さらに，**CaboCha**の解析結果（`neko.txt.cabocha`）を読み込み，**各文をMorphオブジェクトのリスト**として表現し，3文目の形態素列を表示せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 形態素を表すクラス`Morph`を実装せよ\n",
    "class Morph:\n",
    "    \"\"\"\n",
    "    40.「形態素を表すクラスMorphを実装せよ．\n",
    "    このクラスは表層形（surface），基本形（base），\n",
    "    品詞（pos），品詞細分類1（pos1）をメンバ変数に持つこととする」\n",
    "    で使用するクラス。\n",
    "    \"\"\"\n",
    "    def __init__(self, surface : str, base : str,\n",
    "                 pos : str, pos1 : str):\n",
    "\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "\n",
    "        return\n",
    "\n",
    "    def get_surface(self):\n",
    "        return self.surface\n",
    "\n",
    "    def get_base(self):\n",
    "        return self.base\n",
    "\n",
    "    def get_pos(self):\n",
    "        return self.pos\n",
    "\n",
    "    def get_pos1(self):\n",
    "        return self.pos1\n",
    "\n",
    "    def get_list(self) -> list:\n",
    "        return [self.surface, self.base , self.pos , self.pos1]\n",
    "\n",
    "    def get_dict(self) -> dict:\n",
    "        return {\"surface\":self.surface, \"base\":self.base,\n",
    "                \"pos\":self.pos , \"pos1\":self.pos1}\n",
    "\n",
    "    def is_noun(self) -> bool:\n",
    "        return self.pos == \"名詞\"\n",
    "\n",
    "    def is_verb(self) -> bool:\n",
    "        return self.pos == \"動詞\"\n",
    "\n",
    "    def is_adj(self) -> bool:\n",
    "        return self.pos == \"形容詞\"\n",
    "\n",
    "    def is_aux(self) -> bool:\n",
    "        return self.pos == \"助動詞\"\n",
    "\n",
    "    def is_symbol(self) -> bool:\n",
    "        return self.pos == \"記号\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `neko.txt.cabocha`の解析結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS\n",
      "* 0 2D 0/0 -0.764522\n",
      "　\t記号,空白,*,*,*,*,　,　,　\n",
      "* 1 2D 0/1 -0.764522\n",
      "吾輩\t名詞,代名詞,一般,*,*,*,吾輩,ワガハイ,ワガハイ\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "* 2 -1D 0/2 0.000000\n",
      "猫\t名詞,一般,*,*,*,*,猫,ネコ,ネコ\n",
      "で\t助動詞,*,*,*,特殊・ダ,連用形,だ,デ,デ\n",
      "ある\t助動詞,*,*,*,五段・ラ行アル,基本形,ある,アル,アル\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n",
      "EOS\n",
      "* 0 2D 0/1 -1.911675\n",
      "名前\t名詞,一般,*,*,*,*,名前,ナマエ,ナマエ\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "* 1 2D 0/0 -1.911675\n",
      "まだ\t副詞,助詞類接続,*,*,*,*,まだ,マダ,マダ\n",
      "* 2 -1D 0/0 0.000000\n",
      "無い\t形容詞,自立,*,*,形容詞・アウオ段,基本形,無い,ナイ,ナイ\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -20 neko.txt.cabocha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a258c3d13a804ef583decabad1f06984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9977), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# CaboChaの解析結果（neko.txt.cabocha）を読み込み\n",
    "with open('neko.txt.cabocha', encoding='utf-8') as f:\n",
    "    text_sentences = f.read().rstrip().split(\"EOS\")\n",
    "    text_sentences = [[morphs.split(\"\\t\") for morphs in sentence.split(\"\\n\")]\n",
    "                      for sentence in text_sentences]\n",
    "\n",
    "\n",
    "# 各文をMorphオブジェクトのリストとして表現し\n",
    "sentences = list()\n",
    "for sentence in tqdm(text_sentences):\n",
    "    sentence_morph = list()\n",
    "    for line in sentence:\n",
    "        if line[0] != \"\":\n",
    "            if line[0][0] != (\"*\" or \" \" or \"\\u3000\"):\n",
    "                ## Line Example\n",
    "                # [[生れ], [動詞,自立,*,*,一段,連用形,生れる,ウマレ,ウマレ]]\n",
    "                surface = line[0]\n",
    "                others = line[1].split(\",\")\n",
    "                base = others[6]\n",
    "                pos = others[0]\n",
    "                pos1 = others[1]\n",
    "                sentence_morph.append(Morph(surface, base, pos, pos1))\n",
    "\n",
    "    if sentence_morph != []:\n",
    "        sentences.append(sentence_morph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['\\u3000', '\\u3000', '記号', '空白'],\n",
       " ['どこ', 'どこ', '名詞', '代名詞'],\n",
       " ['で', 'で', '助詞', '格助詞'],\n",
       " ['生れ', '生れる', '動詞', '自立'],\n",
       " ['た', 'た', '助動詞', '*'],\n",
       " ['か', 'か', '助詞', '副助詞／並立助詞／終助詞'],\n",
       " ['とんと', 'とんと', '副詞', '一般'],\n",
       " ['見当', '見当', '名詞', 'サ変接続'],\n",
       " ['が', 'が', '助詞', '格助詞'],\n",
       " ['つか', 'つく', '動詞', '自立'],\n",
       " ['ぬ', 'ぬ', '助動詞', '*'],\n",
       " ['。', '。', '記号', '句点']]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_morph_list(sentences: list, idx: int) -> list:\n",
    "\n",
    "    assert idx < len(sentences), \"input index is larger than length of sentences\"\n",
    "    \n",
    "    result = []\n",
    "    for morph in sentences[idx - 1]:\n",
    "        result.append(morph.get_list())\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# 3文目の形態素列を表示せよ\n",
    "get_morph_list(sentences, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
    "\n",
    "---\n",
    "\n",
    "40に加えて，文節を表すクラス`Chunk`を実装せよ．このクラスは形態素（`Morph`オブジェクト）のリスト（`morphs`），係り先文節インデックス番号（`dst`），係り元文節インデックス番号のリスト（`srcs`）をメンバ変数に持つこととする．さらに，入力テキストの`CaboCha`の解析結果を読み込み，１文を`Chunk`オブジェクトのリストとして表現し，8文目の文節の文字列と係り先を表示せよ．第5章の残りの問題では，ここで作ったプログラムを活用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunk:\n",
    "    \"\"\"\n",
    "    41. 「文節を表すクラスChunkを実装せよ．\n",
    "    このクラスは形態素（Morphオブジェクト）のリスト（morphs），\n",
    "    係り先文節インデックス番号（dst），\n",
    "    係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする」\n",
    "    \"\"\"\n",
    "    def __init__(self, morphs : list, dst : int, srcs : list) -> None:\n",
    "\n",
    "        self.morphs = morphs\n",
    "        self.dst = int(dst.strip(\"D\"))\n",
    "        self.srcs = int(srcs)\n",
    "\n",
    "        return\n",
    "\n",
    "    def get_list(self) -> list:\n",
    "        return [self.morphs, self.dst, self.srcs]\n",
    "\n",
    "    def get_dict(self) -> dict:\n",
    "        return {\"morphs\" : self.morphs,\n",
    "                \"dst\" : self.dst,\n",
    "                \"srcs\" : self.srcs}\n",
    "\n",
    "    def get_morphs_text(self) -> list:\n",
    "        return [m.surface for m in self.morphs]\n",
    "\n",
    "    def have_verbs(self):\n",
    "        \n",
    "        return\n",
    "\n",
    "    def have_noun(self):\n",
    "        \n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "def make_chunk_list(file=\"neko.txt.cabocha\") -> list:\n",
    "    \"\"\"\n",
    "    41.「入力テキストのCaboChaの解析結果を読み込み，\n",
    "    １文をChunkオブジェクトのリストとして表現」 の実装。\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    file : str (default: neko.txt.cabocha)\n",
    "        読み込むChasenファイルのパス\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    Chunkオブジェクトのリスト\n",
    "    \"\"\"\n",
    "    sentences = list()\n",
    "    sentence = list()\n",
    "    _chunk = None\n",
    "\n",
    "    with open(file, encoding='utf-8') as f:\n",
    "        for line in tqdm([l.rstrip() for l in f.readlines()],\n",
    "                         desc=\"Making Chunk obj list from Textfile\"):\n",
    "\n",
    "            # chunk deliminater, */EOS\n",
    "            if line[0] == '*':\n",
    "                if _chunk is not None:\n",
    "                    sentence.append(_chunk)\n",
    "                _chunk = Chunk(morphs=[], dst=line.split(\" \")[2], srcs=line.split(\" \")[1])\n",
    "            elif line == 'EOS':\n",
    "                if _chunk is not None:\n",
    "                    # make list of Chunks\n",
    "                    sentence.append(_chunk)\n",
    "                # initialize\n",
    "                if len(sentence) > 0:\n",
    "                    sentences.append(sentence)\n",
    "                _chunk = None\n",
    "                sentence = []\n",
    "            # chunks\n",
    "            else:\n",
    "                line = [line.split('\\t')[0]] + line.split('\\t')[1].split(',')\n",
    "                # line == ['始め', '名詞', '副詞可能', '*', '*', '*', '*', '始め', 'ハジメ', 'ハジメ']\n",
    "                try:\n",
    "                    _morph = Morph(surface=line[0], base=line[7], pos=line[1], pos1=line[2])\n",
    "                except:\n",
    "                    print(line)\n",
    "                # make list of Morphs\n",
    "                _chunk.morphs.append(_morph)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def get_chunk_list(sentences: list, index=None) -> list:\n",
    "    \"\"\"\n",
    "    41.「入力テキストのCaboChaの解析結果を読み込み，\n",
    "    １文をChunkオブジェクトのリストとして表現し，\n",
    "    8文目の文節の文字列と係り先を表示せよ．」で使用\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    sentences : list\n",
    "        Chunkオブジェクトの多次元リスト。\n",
    "        構造は， 文章[[Chunkオブジェクトのリスト]]\n",
    "    index : int (optional)\n",
    "        何番目の文章の文節リストを返すかを指定。\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    chunk_list : list\n",
    "        Sentences->Chunkの構造で，\n",
    "        各文節毎に，[文字列， 係先インデックス， 係元インデックス]\n",
    "        (例)\n",
    "        [[['しかし'], 9, 0],\n",
    "         [['その'], 2, 1],\n",
    "         [['当時', 'は'], 5, 2],\n",
    "         [['何', 'という'], 4, 3],\n",
    "         [['考', 'も'], 5, 4],\n",
    "         [['なかっ', 'た', 'から'], 9, 5],\n",
    "         [['別段'], 7, 6],\n",
    "         [['恐し'], 9, 7],\n",
    "         [['いとも'], 9, 8],\n",
    "         [['思わ', 'なかっ', 'た', '。'], -1, 9]]\n",
    "    \"\"\"\n",
    "    if index is not None:\n",
    "        return [[[m.surface for m in c.morphs], c.dst, c.srcs] for c in sentences[index]]\n",
    "    else:\n",
    "        chunk_list = []\n",
    "        for sentence in tqdm(sentences, desc=\"Making list of Chunks\"):\n",
    "            chunk_list.append([[[m.surface for m in c.morphs], c.dst, c.srcs] for c in sentence])\n",
    "\n",
    "        return chunk_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccf9fa31deb440eda643baa32ad26826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Making Chunk obj list from Textfile', max=297336), HTML(value…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chunk_sentences = make_chunk_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['しかし'], 9, 0],\n",
       " [['その'], 2, 1],\n",
       " [['当時', 'は'], 5, 2],\n",
       " [['何', 'という'], 4, 3],\n",
       " [['考', 'も'], 5, 4],\n",
       " [['なかっ', 'た', 'から'], 9, 5],\n",
       " [['別段'], 7, 6],\n",
       " [['恐し'], 9, 7],\n",
       " [['いとも'], 9, 8],\n",
       " [['思わ', 'なかっ', 'た', '。'], -1, 9]]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_chunk_list(chunk_sentences, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 42. 係り元と係り先の文節の表示\n",
    "\n",
    "---\n",
    "\n",
    "係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Morph at 0x118486630>]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_sentences[5][0].morphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def src_to_dst(chunk_sentences: list) -> list:\n",
    "    \"\"\"\n",
    "    42.「係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．\n",
    "    ただし，句読点などの記号は出力しないようにせよ．」で利用。\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    chunk_sentences : list\n",
    "        Chunkインスタンスのリスト。\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    src_to_dst : list\n",
    "        タブ区切りでの文字列のリスト。\n",
    "\n",
    "    \"\"\"\n",
    "    chunk_sentences = get_chunk_list(chunk_sentences)\n",
    "    src_to_dst = list()\n",
    "\n",
    "    for sentence in tqdm(chunk_sentences, desc=\"Appending Chunks...\"):\n",
    "        for chunk in sentence:\n",
    "            if chunk[1] == -1:\n",
    "                None\n",
    "            else:\n",
    "                src_str = \"\".join(rm_symbol(chunk[0]))\n",
    "                dst_str = \"\".join(sentence[chunk[1]][0])\n",
    "                if is_not_symbol(src_str) and is_not_symbol(dst_str):\n",
    "                    src_to_dst.append(src_str + \"\\t\" + dst_str)\n",
    "\n",
    "    return src_to_dst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d69571ba32149f6a7f2995591268b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Making list of Chunks', max=9222), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd96af8c89c4d858474d7489ea4d420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Appending Chunks...', max=9222), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "list_src_dst = src_to_dst(chunk_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 41983\n",
      "First 5 elements: ['どこで\\t生れたか', '何でも\\t薄暗い', '薄暗い\\t所で', 'じめじめした\\t所で', '所で\\t泣いて']\n"
     ]
    }
   ],
   "source": [
    "check(list_src_dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
    "\n",
    "---\n",
    "\n",
    "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_to_verb(chunk_sentences: list, base=False) -> list:\n",
    "    \"\"\"\n",
    "    43. 「名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．\n",
    "    ただし，句読点などの記号は出力しないようにせよ．」で利用。\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    chunk_sentences : list\n",
    "        Chunkインスタンスのリスト。\n",
    "        \n",
    "    base : bool\n",
    "        基本形で抽出。デフォルトでは表層形で抽出。\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    noun_to_verb : list\n",
    "        タブ区切りでの文字列のリスト。\n",
    "\n",
    "    \"\"\"\n",
    "    noun_to_verb = list()\n",
    "\n",
    "    for sentence in tqdm(chunk_sentences, desc=\"Appending Chunks...\"):\n",
    "        for chunk in sentence:\n",
    "            if chunk.dst == -1:\n",
    "                None\n",
    "            else:\n",
    "                # 係り元 : 名詞を含む\n",
    "                if True in map(Morph.is_noun, chunk.morphs):\n",
    "                    src_str = \"\".join(rm_symbol(list(map(Morph.get_base\n",
    "                                                     if base else Morph.get_surface,\n",
    "                                                     chunk.morphs))))\n",
    "                    \n",
    "                    # 係り先 : 動詞を含む\n",
    "                    dst_morphs = sentence[chunk.dst].morphs\n",
    "                    if True in map(Morph.is_verb, dst_morphs):\n",
    "                        dst_str = \"\".join(rm_symbol(list(map(Morph.get_base\n",
    "                                                    if base else Morph.get_surface,\n",
    "                                                    dst_morphs))))\n",
    "\n",
    "                        noun_to_verb.append(src_str + \"\\t\" + dst_str)\n",
    "\n",
    "    return noun_to_verb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f92794f26546469a2208ab7222c876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Appending Chunks...', max=9222), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "list_noun_to_verb = noun_to_verb(chunk_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 29106\n",
      "First 10 elements: ['どこで\\t生れたか', '見当が\\tつかぬ', '所で\\t泣いて', 'ニャーニャー\\t泣いて', 'いた事だけは\\t記憶している', '吾輩は\\t見た', 'ここで\\t始めて', 'ものを\\t見た', 'あとで\\t聞くと', '我々を\\t捕えて']\n"
     ]
    }
   ],
   "source": [
    "check(list_noun_to_verb, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 44. 係り受け木の可視化\n",
    "\n",
    "---\n",
    "\n",
    "与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，係り受け木をDOT言語に変換し，Graphvizを用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，pydotを使うとよい．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 45. 動詞の格パターンの抽出\n",
    "\n",
    "---\n",
    "\n",
    "今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ．\n",
    "\n",
    "動詞を含む文節において，最左の動詞の基本形を述語とする\n",
    "述語に係る助詞を格とする\n",
    "述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "```\n",
    "始める  で\n",
    "見る    は を\n",
    "このプログラムの出力をファイルに保存し，\n",
    "以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "```\n",
    "\n",
    "コーパス中で頻出する述語と格パターンの組み合わせ\n",
    "「する」「見る」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 46. 動詞の格フレーム情報の抽出\n",
    "\n",
    "---\n",
    "\n",
    "45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
    "\n",
    "項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
    "述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる\n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "```\n",
    "始める  で      ここで\n",
    "見る    は を   吾輩は ものを\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 47. 機能動詞構文のマイニング\n",
    "\n",
    "---\n",
    "\n",
    "動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．\n",
    "\n",
    "「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする\n",
    "述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる\n",
    "述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）\n",
    "例えば「別段くるにも及ばんさと、主人は手紙に返事をする。」という文から，以下の出力が得られるはずである．\n",
    "\n",
    "```\n",
    "返事をする      と に は        及ばんさと 手紙に 主人は\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "```\n",
    "\n",
    "コーパス中で頻出する述語（サ変接続名詞+を+動詞）\n",
    "コーパス中で頻出する述語と助詞パターン\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 48. 名詞から根へのパスの抽出\n",
    "\n",
    "---\n",
    "\n",
    "文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ． ただし，構文木上のパスは以下の仕様を満たすものとする．\n",
    "\n",
    "各文節は（表層形の）形態素列で表現する\n",
    "パスの開始文節から終了文節に至るまで，各文節の表現を`\"->\"`で連結する\n",
    "「吾輩はここで始めて人間というものを見た」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
    "\n",
    "```\n",
    "吾輩は -> 見た\n",
    "ここで -> 始めて -> 人間という -> ものを -> 見た\n",
    "人間という -> ものを -> 見た\n",
    "ものを -> 見た\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 49. 名詞間の係り受けパスの抽出\n",
    "\n",
    "---\n",
    "\n",
    "文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号がiとj（i<j）のとき，係り受けパスは以下の仕様を満たすものとする．\n",
    "\n",
    "- 問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を\"->\"で連結して表現する\n",
    "\n",
    "\n",
    "- 文節iとjに含まれる名詞句はそれぞれ，XとYに置換する\n",
    "\n",
    "\n",
    "また，係り受けパスの形状は，以下の2通りが考えられる．\n",
    "\n",
    "- 文節iから構文木の根に至る経路上に文節jが存在する場合: 文節iから文節jのパスを表示\n",
    "\n",
    "\n",
    "- 上記以外で，文節iと文節jから構文木の根に至る経路上で共通の文節kで交わる場合: 文節iから文節kに至る直前のパスと文節jから文節kに至る直前までのパス，文節kの内容を\"|\"で連結して表示\n",
    "\n",
    "\n",
    "例えば，「吾輩はここで始めて人間というものを見た。」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
    "\n",
    "```\n",
    "Xは | Yで -> 始めて -> 人間という -> ものを | 見た\n",
    "Xは | Yという -> ものを | 見た\n",
    "Xは | Yを | 見た\n",
    "Xで -> 始めて -> Y\n",
    "Xで -> 始めて -> 人間という -> Y\n",
    "Xという -> Y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
