{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 言語処理100本ノック\n",
    "# 第8章: 機械学習\n",
    "\n",
    "---\n",
    "\n",
    "本章では，Bo Pang氏とLillian Lee氏が公開している`Movie Review Dataのsentence polarity dataset v1.0`を用い，文を肯定的（ポジティブ）もしくは否定的（ネガティブ）に分類するタスク（極性分析）に取り組む．\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt-polaritydata/ has already been exist!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"rt-polaritydata/\"):\n",
    "    print(\"rt-polaritydata/ has already been exist!\")\n",
    "else:\n",
    "    os.system(\"wget -O rt-polaritydata.tar.gz http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\")\n",
    "    os.system(\"tar -zxvf rt-polaritydata.tar.gz\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: -l: No such file or directory\n",
      "rt-polaritydata/:\n",
      "rt-polarity.neg  rt-polarity.pos\n"
     ]
    }
   ],
   "source": [
    "ls rt-polaritydata/ -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
      "the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth . \n",
      "effective but too-tepid biopic\n",
      "simplistic , silly and tedious . \n",
      "it's so laddish and juvenile , only teenage boys could possibly find it funny . \n",
      "exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -3 rt-polaritydata/rt-polarity.pos\n",
    "\n",
    "head -3 rt-polaritydata/rt-polarity.neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "nkf -w --overwrite rt-polaritydata/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(l: list, head=5) -> None:\n",
    "    \"\"\"\n",
    "    Data Helper check()\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    l: list\n",
    "        list that you want to check\n",
    "\n",
    "    head: int\n",
    "        size, length of sample log output\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    output sentence for checking list object\n",
    "    structure and size.\n",
    "    \"\"\"\n",
    "    output = \"\"\n",
    "    output += \"Length: {}\\n\".format(len(l))\n",
    "    output += \"Shape : {}\\n\".format(np.shape(l))\n",
    "    output += \"First {} elements: {}\".format(head, l[:head])\n",
    "    print(output)\n",
    "    return\n",
    "\n",
    "def tokenize(sentence: str) -> list:\n",
    "    return sentence.split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 70. データの入手・整形\n",
    "\n",
    "文に関する極性分析の正解データを用い，以下の要領で正解データ（sentiment.txt）を作成せよ．\n",
    "\n",
    "`rt-polarity.pos`の各行の先頭に**\"+1 \"**という文字列を追加する（極性ラベル\"+1\"とスペースに続けて肯定的な文の内容が続く）\n",
    "`rt-polarity.neg`の各行の先頭に**\"-1 \"**という文字列を追加する（極性ラベル\"-1\"とスペースに続けて否定的な文の内容が続く）\n",
    "上述1と2の内容を結合（concatenate）し，行をランダムに並び替える\n",
    "`sentiment.txt`を作成したら，正例（肯定的な文）の数と負例（否定的な文）の数を確認せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_concat(pos=\"./rt-polaritydata/rt-polarity.pos\",\n",
    "                  neg=\"./rt-polaritydata/rt-polarity.neg\",\n",
    "                  seed=1, output_file_name=\"sentiment.txt\") -> tuple:\n",
    "    \"\"\"\n",
    "    70.「rt-polarity.posの各行の先頭に\"+1 \"という文字列を追加する\n",
    "    （極性ラベル\"+1\"とスペースに続けて肯定的な文の内容が続く）\n",
    "    rt-polarity.negの各行の先頭に\"-1 \"という文字列を追加する\n",
    "    （極性ラベル\"-1\"とスペースに続けて否定的な文の内容が続く）\n",
    "    上述1と2の内容を結合（concatenate）し，行をランダムに並び替える\n",
    "    sentiment.txtを作成したら，正例（肯定的な文）の数と負例（否定的な文）の数を確認せよ．」\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    pos : str\n",
    "        file path to pos-data text file\n",
    "    neg : str\n",
    "        negative one\n",
    "    seed : int (default: 1)\n",
    "        seed value for random function\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    output : str\n",
    "        randomly concatenated text.\n",
    "    \"\"\"\n",
    "    \n",
    "    labels = list()\n",
    "\n",
    "    with open(pos, encoding=\"utf-8\") as p:\n",
    "        pos = [\"+1\" + \" \" + str(line.rstrip()) for line in p.readlines()]\n",
    "\n",
    "    with open(neg, encoding=\"utf-8\") as n:\n",
    "        neg = [\"-1\" + \" \" + str(line.rstrip()) for line in n.readlines()]\n",
    "\n",
    "    output = pos + neg\n",
    "    \n",
    "    random.seed(seed)\n",
    "    random.shuffle(output)\n",
    "\n",
    "    labels = [1] * len(pos) + [0] * len(neg)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(labels)\n",
    "    \n",
    "    print(\"Randomized text list.\")\n",
    "    check(output, head=1)\n",
    "\n",
    "    with open(output_file_name, \"w\") as w:\n",
    "        w.writelines(output)\n",
    "\n",
    "    return \"\\n\".join(output), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized text list.\n",
      "Length: 10661\n",
      "First 1 elements: ['-1 a negligible british comedy .']\n",
      "positive file P=5330\n",
      "negative file N=5331\n"
     ]
    }
   ],
   "source": [
    "sentiment, labels = random_concat()\n",
    "\n",
    "# 正例（肯定的な文）の数と負例（否定的な文）の数を確認せよ．\n",
    "print(\"positive file P={}\".format(labels.count(1)))\n",
    "print(\"negative file N={}\".format(labels.count(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# len(list(map(bool, labels)))\n",
    "# pd.Series(sentiment.split(\"\\n\"))[(list(map(bool, labels)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 71. ストップワード\n",
    "\n",
    "英語のストップワードのリスト（ストップリスト）を適当に作成せよ．さらに，引数に与えられた単語（文字列）がストップリストに含まれている場合は真，それ以外は偽を返す関数を実装せよ．さらに，その関数に対するテストを記述せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "stop_words = [\"at\", \"in\", \"on\", \"by\", \"with\", \"of\",\n",
    "              \"I\", \"you\", \"am\", \"are\", \"it\", \"is\",\n",
    "              \"this\", \"that\", \"these\", \"those\", \"the\"]\n",
    "\n",
    "\n",
    "def is_stop_word(word : str) -> bool:\n",
    "    \"\"\"\n",
    "    英語のストップワードのリスト（ストップリスト）を適当に作成せよ．\n",
    "    さらに，引数に与えられた単語（文字列）がストップリストに含まれている場合は真，\n",
    "    それ以外は偽を返す関数を実装せよ．さらに，その関数に対するテストを記述せよ．\n",
    "    \"\"\"\n",
    "    return word in stop_words\n",
    "\n",
    "\n",
    "def have_stop_word(word : list) -> bool:\n",
    "    \"\"\"\n",
    "    英語のストップワードのリスト（ストップリスト）を適当に作成せよ．\n",
    "    さらに，引数に与えられた単語（文字列）がストップリストに含まれている場合は真，\n",
    "    それ以外は偽を返す関数を実装せよ．さらに，その関数に対するテストを記述せよ．\n",
    "    \"\"\"\n",
    "    if type(word) == str:\n",
    "        word = tokenize(word)\n",
    "    \n",
    "    return (True in [stop_word in word for stop_word in stop_words])\n",
    "\n",
    "\n",
    "def remove_stop_word(words : list) -> list:\n",
    "    \n",
    "    if type(words) == str:\n",
    "        words = tokenize(words)\n",
    "    \n",
    "    for sw in stop_words:\n",
    "        if sw in words:\n",
    "            words.remove(sw)\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "# check it\n",
    "print(is_stop_word(\"this\"))\n",
    "print(have_stop_word(\"the gorgeously elaborate continuation of\"))\n",
    "print(have_stop_word(\"gorgeously elaborate continuation\"))\n",
    "print(have_stop_word(remove_stop_word(\"the gorgeously elaborate continuation of\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 72. 素性抽出\n",
    "\n",
    "極性分析に有用そうな素性を各自で設計し，学習データから素性を抽出せよ．\n",
    "素性としては，レビューからストップワードを除去し，各単語をステミング処理したものが最低限のベースラインとなるであろう．\n",
    "\n",
    "- ストップワード  \n",
    "\n",
    "上記，要らん＝共通して大量に出現する単語を除去。\n",
    "\n",
    "- ステミング処理  \n",
    "\n",
    "語幹を揃える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/atsuya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/atsuya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# using NLTK (Natural Language Toolkit)\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenizing\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# Stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "# sci-kit learn\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "class feature_extractor:\n",
    "    \"\"\"\n",
    "    8章-72\n",
    "    「極性分析に有用そうな素性を各自で設計し，学習データから素性を抽出せよ． \n",
    "    素性としては，レビューからストップワードを除去し，\n",
    "    各単語をステミング処理したものが最低限のベースラインとなるであろう．」\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data) -> None:\n",
    "        \"\"\"\n",
    "        データのロード等。\n",
    "        \n",
    "        PARAM\n",
    "        -----\n",
    "        data : str or list or FILEPATH\n",
    "        \n",
    "        RETURN -> None\n",
    "        ------\n",
    "        set feature-extracted data as instance member.\n",
    "        \n",
    "        input data format\n",
    "        -----------------\n",
    "        +1 positive sentence about film....\n",
    "        -1 negative sentence about film....\n",
    "        +1 positive sentence about film....\n",
    "        +1 positive sentence about film....\n",
    "        -1 negative sentence about film....\n",
    "        :\n",
    "        :\n",
    "        \"\"\"\n",
    "        if type(data) == str:\n",
    "            \n",
    "            if data[-4:] == \".txt\" and os.path.exists(data):\n",
    "                with open(data, encoding=\"utf-8\") as f:\n",
    "                    self.list_data = [line.rstrip()\n",
    "                                      for line in tqdm(f.readlines(),\n",
    "                                                       desc=\"Importing...\")]\n",
    "                    self.text_data = f.read()\n",
    "\n",
    "            else:\n",
    "                self.text_data = data\n",
    "                self.list_data = data.splitlines()\n",
    "\n",
    "        elif type(data) == list:\n",
    "\n",
    "            self.list_data = data\n",
    "            self.text_data = \"\\n\".join(data)\n",
    "\n",
    "        else:\n",
    "            return \"Input data should be str or list.\"\n",
    "\n",
    "        labels = []\n",
    "        for l in tqdm(self.list_data, desc=\"Extract Labels\"):\n",
    "            labels.append(l[0])\n",
    "        self.labels = labels\n",
    "\n",
    "        return\n",
    "\n",
    "    def check(self, l=1) -> None:\n",
    "        check(self.list_data, l)\n",
    "        return\n",
    "\n",
    "    def list2text(self, data: list=None) -> str:\n",
    "    \n",
    "        if data is None:\n",
    "            data = self.list_data\n",
    "\n",
    "        output = list()\n",
    "        for sentence in tqdm(data, desc=\"List to String...\"):\n",
    "            output.append(\" \".join(sentence))\n",
    "        output = \"\\n\".join(output)\n",
    "    \n",
    "        return output\n",
    "\n",
    "    def shuffle(self, seed=1) -> None:\n",
    "        random.seed(seed)\n",
    "        random.shuffle(self.list_data)\n",
    "        random.seed(seed)\n",
    "        random.shuffle(self.labels)\n",
    "        self.text_data = self.list2text(self.list_data)\n",
    "        return\n",
    "\n",
    "    def tokenize(self) -> None:\n",
    "        \"\"\"\n",
    "        Tokenize list data.\n",
    "        each elements will be separated to list of strings.\n",
    "        \"\"\"\n",
    "        if type(self.list_data[0]) is list:  # is_tokenized\n",
    "            return None  # Already tokenized\n",
    "\n",
    "        self.list_data = [nltk.tokenize.word_tokenize(l)\n",
    "                          for l in tqdm(self.list_data,\n",
    "                                        desc=\"Tokenizing...\")] \n",
    "        return\n",
    "\n",
    "    def stemming(self) -> None:\n",
    "        \"\"\"\n",
    "        Stemming data.\n",
    "        each word of text will be stemmed.\n",
    "        \"\"\"\n",
    "        if type(self.list_data[0]) is str:  # is_tokenized\n",
    "            self.tokenize()\n",
    "        \n",
    "        stemmer = PorterStemmer()\n",
    "    \n",
    "        stemmed_list_data = list()\n",
    "        for sentence in tqdm(self.list_data, desc=\"Stemming...\"):\n",
    "            stemmd_sentence = list()\n",
    "            for word in sentence:\n",
    "                stemmd_sentence.append(stemmer.stem(word))\n",
    "            stemmed_list_data.append(stemmd_sentence)\n",
    "            self.list_data = stemmed_list_data\n",
    "        return\n",
    "    \n",
    "    def remove_sw(self):\n",
    "        \"\"\"\n",
    "        remove StopWords.\n",
    "        \"\"\"\n",
    "        if type(self.list_data[0]) is str:  # is_tokenized\n",
    "            self.tokenize()\n",
    "        # Load StopWords list from nltk corpus data.\n",
    "        self.sw_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "        rmsw_list_data = list()\n",
    "        sw_cnt = 0\n",
    "        for sentence in tqdm(self.list_data, desc=\"Removing SW...\"):\n",
    "            rmsw_sentence = list()\n",
    "            for word in sentence:\n",
    "                if word in self.sw_list:\n",
    "                    sw_cnt += 1\n",
    "                else:\n",
    "                    rmsw_sentence.append(word)\n",
    "            rmsw_list_data.append(rmsw_sentence)\n",
    "            self.list_data = rmsw_list_data\n",
    "        print(\"Total {} StopWords detected & removed\".format(sw_cnt))\n",
    "\n",
    "        return\n",
    "\n",
    "    def remove_label(self) -> None:\n",
    "        if \"+1\" in self.list_data[0]:  # check already removed or not\n",
    "            self.list_data = [s[1:] for s in self.list_data]\n",
    "        return\n",
    "\n",
    "    def clean(self) -> None:\n",
    "        self.tokenize()\n",
    "        self.stemming()\n",
    "        self.remove_sw()\n",
    "        return\n",
    "    \n",
    "    def vectorize(self):\n",
    "        self.remove_label()\n",
    "        data = self.list2text()\n",
    "        count_vectorizer = CountVectorizer()\n",
    "        self.vector = count_vectorizer.fit_transform(data)\n",
    "        print('word数: ' + len(self.vector.toarray()[0]))\n",
    "    \n",
    "    def extract(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb847719f09471ea887f99defaa52aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Extract Labels', max=10661), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = feature_extractor(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3085461c2db74678b87c8a381b7b8da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Tokenizing...', max=10661), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041185d2b1ad4495a56206cec40e42bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Removing SW...', max=10661), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 85484 StopWords detected & removed\n",
      "Length: 10661\n",
      "Shape : (10661,)\n",
      "First 5 elements: [['-1', 'negligible', 'british', 'comedy', '.'], ['-1', 'densest', 'distillation', 'roberts', \"'\", 'movies', 'ever', 'made', '.'], ['+1', 'even', \"'ve\", 'seen', '``', 'stomp', '``', '(', 'stage', 'show', ')', ',', 'still', 'see', '!'], ['-1', 'incomprehensible', 'mess', 'feels', 'less', 'like', 'bad', 'cinema', 'like', 'stuck', 'dark', 'pit', 'nightmare', 'bad', 'cinema', '.'], ['+1', 'idealistic', 'love', 'story', 'brings', 'latent', '15-year-old', 'romantic', 'everyone', '.']]\n"
     ]
    }
   ],
   "source": [
    "data.remove_sw()\n",
    "data.check(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68c7952b87547c18f44ebe590aeaac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Stemming...', max=10661), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "044c20f6f24d4e1e9b464c7c9955efe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Removing SW...', max=10661), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 103 StopWords detected & removed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c764d1bbc09c4f909d24dba3b4b1fcd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='List to String...', max=10661), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 10661\n",
      "Shape : (10661,)\n",
      "First 1 elements: [['one', 'best', ',', 'underst', 'perform', '[', 'jack', 'nicholson', \"'s\", ']', 'career', '.']]\n"
     ]
    }
   ],
   "source": [
    "data.clean()\n",
    "data.shuffle()\n",
    "data.remove_label()\n",
    "data.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 10661\n",
      "Shape : (10661,)\n",
      "First 5 elements: [['one', 'best', ',', 'underst', 'perform', '[', 'jack', 'nicholson', \"'s\", ']', 'career', '.'], ['look', 'good', ',', 'essenti', 'empti', '.'], ['movi', 'lack', 'action', 'make', 'drama', ',', 'suspens', ',', 'reveng', ',', 'romanc', '.'], ['rabbit-proof', 'fenc', 'probabl', 'make', 'angri', '.', 'like', 'make', 'weep', ',', 'way', \"n't\", 'make', 'feel', 'like', 'sucker', '.'], ['whenev', 'think', \"'ve\", 'figur', 'late', 'marriag', ',', 'throw', 'loop', '.']]\n"
     ]
    }
   ],
   "source": [
    "data.remove_label()\n",
    "data.check(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe0aec204c24b8f8d768edf54887699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='List to String...', max=10661), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Iterable over raw text documents expected, string object received.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-255-4871abee5d64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-250-cb16a537df88>\u001b[0m in \u001b[0;36mvectorize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist2text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mcount_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'word数: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             raise ValueError(\n\u001b[0;32m--> 860\u001b[0;31m                 \u001b[0;34m\"Iterable over raw text documents expected, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m                 \"string object received.\")\n\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
     ]
    }
   ],
   "source": [
    "data.vectorize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 73. 学習\n",
    "\n",
    "72で抽出した素性を用いて，ロジスティック回帰モデルを学習せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 74. 予測\n",
    "73で学習したロジスティック回帰モデルを用い，与えられた文の極性ラベル（正例なら\"+1\"，負例なら\"-1\"）と，その予測確率を計算するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 75. 素性の重み\n",
    "73で学習したロジスティック回帰モデルの中で，重みの高い素性トップ10と，重みの低い素性トップ10を確認せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 76. ラベル付け\n",
    "学習データに対してロジスティック回帰モデルを適用し，正解のラベル，予測されたラベル，予測確率をタブ区切り形式で出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 77. 正解率の計測\n",
    "76の出力を受け取り，予測の正解率，正例に関する適合率，再現率，F1スコアを求めるプログラムを作成せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 78. 5分割交差検定\n",
    "76-77の実験では，学習に用いた事例を評価にも用いたため，正当な評価とは言えない．すなわち，分類器が訓練事例を丸暗記する際の性能を評価しており，モデルの汎化性能を測定していない．そこで，5分割交差検定により，極性分類の正解率，適合率，再現率，F1スコアを求めよ．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 79. 適合率-再現率グラフの描画\n",
    "\n",
    "ロジスティック回帰モデルの分類の閾値を変化させることで，適合率-再現率グラフを描画せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
